<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0046)http://leap.ee.iisc.ac.in/prachi/research.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<link rel="stylesheet" type="text/css" href="./css/acl.css">
  


 <title>Prachi Singh</title>
  
  



<!-- <style>

figure{
font-style: italic;
  font-size: smaller;
}
.collapsible {
  margin: 0px 20px;
  background-color: #FFFFFF;
  color: #005CA8;
  cursor: pointer;
  padding: 1px;
  width: 80%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #FFFFFF;
}

.content {
  margin: 0px 20px;
  padding: 0 1px;
  display: none;
  overflow: hidden;
  background-color: #e8f4ff;
}
p {
margin: 4px 4px;
}

</style> -->
 
</head>

<body>
<!--center everything-->
<div id="headwrapper">

	<!--Header-->
	<div id="header"> 
		
		<!-- LOGO -->
		<!--
		<div id="logo"><div id="LSE">AKS</div><div id="econ">Auditory NeuroScience</div></div> -->

		<!--Name-->
		
		<div id="name">Prachi Singh</div>
		<div id="affiliation">Research Scholar<br></div>
		<div id="navi">	<a href="index.html"><div id="off">About me</div></a>
						<a href="research.html"><div id="on">Research</div></a>
						<a href="cv.html"><div id="off">CV</div></a>
						<a href="publications.html"><div id="off">Publications</div></a>
						<a href="extras.html"><div id="off">Extras</div></a>
		</div>
	</div>
	<!--end Header-->
</div>
<div id="outerwrapper">
	<!--Content-->
	<div id="maincontent">
		<div id="onecol">
			<div id="subheader">Overview</div>
		<br><strong><font color="orange">Research Interests:</font></strong><br>
		Machine Learning, Speech Processing, Speaker Diarization, Signal Processing, Variational Inference, Self-supervised Learning, Graph based Clustering.
	   
			<div id="subheader">Thesis Work </div>
			<div id="subsubheader">Graph Clustering Approaches for Speaker Diarization</div>
			
			My research involves self-supervised and supervised clustering approaches for automatic speaker diarization in the context of conversational speech.
			<div id="subsubheader">What is Speaker Diarization ?</div>
			It is the task of partitioning an audio containing multiple speakers into segments assigning labels to the corresponding speakers.  
			<figure>
				<img id="landscape" src="./images/googlediarize.gif">
				<figcaption>Different colours represent different speakers. Courtesy :<a href="https://ai.googleblog.com/2018/11/accurate-online-speaker-diarization.html">Google Blog</a>
			</figcaption></figure>
			<!--
			We also identify the brain regions (based on EEG channels) that contain the most language discriminative cues as the temporal lobe and the frontal lobe.
		-->

			<!-- PAPER LINK 
			<div id="subsubheader">Download:</div>
			<div id="paperlink"><a href="" id="paperlink">CEP Discussion Paper 0000</a></div>
			-->
			
			<li>Graph Neural Network based speaker diarization [<a href="">webpage</a>][<a href="">paper1</a>] [<a href="https://arxiv.org/pdf/2109.06824.pdf">paper2</a>]
				<button type="button" class="collapsible">Abstract</button>
				<div class="content">
				<p> Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications.  
					The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion.
					While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical  Clustering (E-SHARC). 
					The embedding extractor is initialized using a pre-trained x-vector model while the GNN model is trained initially using the x-vector embeddings from the pre-trained model. 
					Finally, the E-SHARC model uses the  front-end mel-filterbank features as input and jointly optimizes the embedding extractor and the GNN clustering module,  performing representation learning, metric learning, and clustering with end-to-end optimization.
					Further, with additional inputs from an external overlap detector, the E-SHARC approach is capable of predicting the  speakers in the overlapping speech regions. 
					The experimental evaluation on benchmark datasets like AMI, Voxconverse and DISPLACE, 
					illustrates that the proposed E-SHARC framework provides competitive diarization results using graph based clustering methods.       
		
				</p> 
				</div></li>
			<li>Self-supervised speaker diarization with path integral clustering [<a href="https://arxiv.org/pdf/2104.09456.pdf">paper1</a>][<a href="https://arxiv.org/pdf/2109.06824.pdf">paper2</a>]
					<button type="button" class="collapsible">Abstract</button>
					<div class="content">
					<p> This work involves learning representations using clustering based loss. 
						The task is self-supervised because we learn the representations using the clustering output given by the clustering algorithm
						 to make the representations more speaker discriminative.
						  We explored graph structural path integral clustering to encode embedding space in the 
						  form of graph. Published in IEEE Transactions on Speech, Audio and Language Processing.

					</p> 
					</div></li>

			<li>Self-Supervised Speaker Diarization [<a href="http://leap.ee.iisc.ac.in/prachi/arxiv.org/pdf/2008.03960.pdf">paper</a>]
				<button type="button" class="collapsible">Abstract</button>
				<div class="content">
				<p> The proposed approach is based on principles of self-supervised learning where the self-supervision is derived from the clustering algorithm. 
					The representations are learnt using triplet based loss derived from clustering output from previous stage. 
					The work is accepted in Interspeech 2020.
				</p> 
				</div></li>
			<li>Third DIHARD speech diarization challenge  [<a href="">paper</a>]
					<button type="button" class="collapsible">Abstract</button>
					<div class="content">
					<p> Contributed in baseline system setup for the DIHARD-III challenge. 
						It involves task to partition an audio into speaker segments, in challenging environment where the audio is corrupted with noise, music, babble etc. and contains short speaker turns.
						It has applications in rich-text transcription of meetings, clinical diagnosis etc.
						Participated in challenge and was among top 10 teams across globe. Our system involved combination of End-to-End diarization based on transformers 
						for telephone conversation and graph based clustering for multi-speaker conversations.
					</p> 
					</div></li>
			<li>Speaker Diarization using Posterior Scaled VB-HMM  [<a href="https://arxiv.org/pdf/2104.02359.pdf">paper1</a>] [<a href="./publications/DIHARD_2019_challenge_Prachi.pdf">paper2</a>]
				<button type="button" class="collapsible">Abstract</button>
			<div class="content">
			<p>The project involves identifying different speakers present in different segment of a given audio recording from DIHARD dataset which has challenging scenarios including restaurants, clinical interviews, mother child conversations etc. using posterior scaled Variational Bayes - Hidden Markov Model. 
				The work is published in Interspeech 2019.
			</p>
</div></li>
			
			<li>Diarization for multi-speaker test conditions in SRE 2018/2019 challenge <a href="./publications/icassp_leap_sre18.pdf">paper1</a>] [<a href="./publications/Odyssey2020_SRE.pdf">paper2</a>]
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                    <p>SRE 2018/2019 challenge involved test conditions with multiple speaker. We perform diarization to extract individual speaker segments to score against the enrollment.
This work is published in ICASSP 2019.</p>
				</div></li>
<div id="subheader">Other Research Work </div>	
Audio Retrieval For Multimodal Design Documents: A New Dataset And Algorithms [<a href="https://prachiisc.github.io/doc2audio/">webpage</a>]
	<button type="button" class="collapsible">Abstract</button>
	<div class="content">
	<p> We consider and propose a new problem of retrieving audio files relevant to multimodal design document inputs comprising both textual elements and visual imagery, e.g., birthday/greeting cards. 
		In addition to enhancing user experience, integrating audio that matches the theme/style of these inputs also helps improve the accessibility of these documents (e.g., visually impaired people can listen to the audio instead). 
		While recent work in audio retrieval exists, these methods and datasets are targeted explicitly towards natural images. However, our problem considers multimodal design documents (created by users using creative software) substantially different from a naturally clicked photograph. To this end, our first contribution is collecting and curating a new large-scale dataset called Melodic-Design (or MELON), comprising design documents representing various styles, themes, templates, illustrations, etc., paired with music audio.	</p> 
	</div>
	

<div id="subheader"> Workshops and Conferences <!-- <em>aka</em> Partners-in-Crime: --></div>
				<ul>
					<li> Presented in ICASSP 2023, Greece
					</li><li> Presented in IISc EECS Symposium April,2022
					</li><li> Presented paper in ASRU 2021
					</li><li> Presented in IISc EECS Symposium May,2021
					</li><li> Presented in IEEE-IISc Shannon's Day talk series, April,2021
					</li><li> <a href="https://www.youtube.com/watch?v=_QvARv_vKA8&amp;list=PLK8w8IgaxTVrf1DBMajNytq87bZi183El&amp;index=9" target="_blank">Presented in DIHARD-III challenge workshop 2020</a>
					</li><li> <a href="https://www.youtube.com/watch?v=UHkDulpSOyE" target="_blank">Talk on Women in Research in PyConIndia 2020, Online</a>
					</li><li> Winter School on Speech and Audio Processing (WiSSAP) 2020,IIT Mandi, India
					</li><li> Presented paper and poster in Interspeech 2019, Graz, Austria 
					</li><li> Summer school on mathematics for data science 2019 organised by IFCAM and IISc
					</li><li> Winter School on Speech and Audio Processing (WiSSAP) 2019, Trivandrum, India
					</li><li> Interspeech 2018, Hyderabad, India
					</li><li> Brain Computation and Learning Workshop, 2018, Bangalore, India
					</li><li> International Conference on Signal Processing and Communications(SPCOM), 2018
				</li></ul>
			


		<!--Foot-->
	<!--div id="fooder"><div id="child">reach me at: prachisingh AT iisc DOT ac DOT in <br />
					   Postal: C328 (LEAP Lab) / Department of Electrical Engineering / IISc Bangalore / India 560012</div></div-->
	<!--end Foot-->

<!--end center everything-->
<script src="./js/app.js"></script>
<noscript>You need to enable JavaScript to view the full site.</noscript>
<!-- <script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script> -->



</div></div></div></body></html>